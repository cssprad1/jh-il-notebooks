{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression - Multi-node, Multi-GPU\n",
    "\n",
    "Note: The tools used in this demo are still classified as experimental by the RAPIDS-AI team. \n",
    "\n",
    "\n",
    "RAPIDS leverages Dask to do embarrassingly-parallel model fitting. For a random forest with N trees fit by K workers, each worker will build and fit N / K trees. \n",
    "\n",
    "In order to build an accurate RF regressor, it is necessary to ensure that each worker has a representitive partition of the data. One route would be to evenly distribute properly-shuffled data. If the GPU cluster has enough working memory, the caller may replicate the entirety of the data to all workers. \n",
    "\n",
    "Look [here](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.dask.ensemble.RandomForestRegressor) for more informationr regarding distributed RF regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cuml\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn import model_selection\n",
    "\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestRegressor as cumlDaskRF\n",
    "from sklearn.ensemble import RandomForestRegressor as sklRF\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Cluster\n",
    "\n",
    "Here Dask forms its own local \"cluster\", which will use all GPUs on the local host by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "c = Client(cluster)\n",
    "\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters\n",
    "\n",
    "Here we will set parameters for our random forest building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 80\n",
    "n_bins = 16\n",
    "n_trees = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#df_pandas = pd.read_csv('Data/train_validate_10_final.csv').astype(np.float32)\n",
    "df_pandas = pd.read_csv('Data/aviris_bands_extract_final.csv').astype(np.float32) # Read the csv into a pandas DataFrame\n",
    "\n",
    "cpu_training_predictors = df_pandas['depth_m']\n",
    "cpu_covariates = df_pandas.drop(['depth_m'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "                                                            cpu_covariates,\n",
    "                                                            cpu_training_predictors,\n",
    "                                                            shuffle=True,\n",
    "                                                            train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions = n_workers\n",
    "\n",
    "def distribute(X, y):\n",
    "    X_cudf = cudf.DataFrame.from_pandas(pd.DataFrame(X))\n",
    "    y_cudf = cudf.Series(y)\n",
    "    \n",
    "    #Partition with Dask\n",
    "    # Workers will train on 1/n_partitions of the data\n",
    "    X_dask = dask_cudf.from_cudf(X_cudf, npartitions=n_partitions)\n",
    "    y_dask = dask_cudf.from_cudf(y_cudf, npartitions=n_partitions)\n",
    "    \n",
    "    # Persis to cache data in active memory\n",
    "    X_dask, y_dask = \\\n",
    "        dask_utils.persist_across_workers(c, [X_dask, y_dask], workers=workers)\n",
    "    \n",
    "    return X_dask, y_dask\n",
    "\n",
    "X_train_dask, y_train_dask = distribute(X_train, y_train)\n",
    "X_test_dask, y_test_dask = distribute(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn model (for comparison)\n",
    "\n",
    "Sci-kit does offer multi-CPU support via joblib.\n",
    "\n",
    "Note: Be wary that if using the large 390-band dataset, an sklearn model may take up to 30 minutes to fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "skl_model = sklRF(max_depth=max_depth, n_estimators=n_trees, n_jobs=-1)\n",
    "skl_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Dask-cuML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cuml_model = cumlDaskRF(max_depth=max_depth, n_estimators=n_trees, n_bins=n_bins, n_streams=n_streams)\n",
    "cuml_model.fit(X_train_dask, y_train_dask)\n",
    "\n",
    "wait(cuml_model.rfs) # Allow async tasks to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_y_pred = skl_model.predict(X_test)\n",
    "cuml_y_pred = cuml_model.predict(X_test_dask).compute().to_array()\n",
    "\n",
    "print(\"SKLearn mean_absolute_erroror: \", mean_absolute_error(y_test, skl_y_pred))\n",
    "print(\"CuML mean_absolute_error: \", mean_absolute_error(y_test, cuml_y_pred))\n",
    "\n",
    "print(\"SKLearn r^2: \", r2_score(y_test, skl_y_pred))\n",
    "print(\"CuML r^2: \", r2_score(y_test, cuml_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may notice, the r^2 score for the cuML-Dask model is negative. This is actually a positive score. This happens when the cuML predictions are measured against host-bound data. This is believed to be due to how data is store/represented in GPUs vs CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rapids-0.16]",
   "language": "python",
   "name": "conda-env-.conda-rapids-0.16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
